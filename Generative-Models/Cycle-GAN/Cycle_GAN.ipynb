{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cycle-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrB8dJgOgy8H",
        "outputId": "bec97521-4a14-4a86-c374-bdb6f17635c4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3QQsSSMlA7r",
        "outputId": "f8b4490d-254a-441c-e3bc-75481883b34d"
      },
      "source": [
        "pip install tensorflow_addons"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\r\u001b[K     |▌                               | 10kB 26.7MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 15.0MB/s eta 0:00:01\r\u001b[K     |█▍                              | 30kB 13.1MB/s eta 0:00:01\r\u001b[K     |█▉                              | 40kB 12.2MB/s eta 0:00:01\r\u001b[K     |██▎                             | 51kB 7.7MB/s eta 0:00:01\r\u001b[K     |██▉                             | 61kB 7.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 71kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▊                            | 81kB 9.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 102kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 112kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 122kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 133kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 143kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 153kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 163kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 174kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 184kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 194kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 204kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 215kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 225kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 235kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 245kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 256kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 266kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 276kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 286kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 296kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 307kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 317kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 327kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 337kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 348kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 358kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 368kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 378kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 389kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 399kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 409kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 419kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 430kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 440kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 450kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 460kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 471kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 481kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 491kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 501kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 512kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 522kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 532kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 542kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 552kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 563kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 573kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 583kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 593kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 604kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 614kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 624kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 634kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 645kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 655kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 665kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 675kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 686kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 696kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 706kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.12.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6RdGpPQkdWk"
      },
      "source": [
        "import tensorflow as tf \n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtFSJxn4F9_8"
      },
      "source": [
        "class InputPipeline:\n",
        "  def __init__(self,orig_height=286,\n",
        "               orig_width=286,\n",
        "               new_height=256,\n",
        "               new_width=256,\n",
        "               batch_size=1):\n",
        "    self.orig_height = orig_height\n",
        "    self.orig_width = orig_width\n",
        "    self.new_height = new_height\n",
        "    self.new_width = new_width\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  @staticmethod\n",
        "  def load_data():\n",
        "    dataset, metadata = tfds.load('cycle_gan/horse2zebra',\n",
        "                                with_info=True,\n",
        "                                as_supervised = True)\n",
        "    train_horses, train_zebras = dataset['trainA'],dataset['trainB']\n",
        "    test_horses, test_zebras = dataset['testA'], dataset['testB']\n",
        "    return train_horses, train_zebras, test_horses, test_zebras\n",
        "\n",
        "  def train_preprocess_image(self,image,label):\n",
        "    image = tf.image.resize(image, [self.orig_height,\n",
        "                                    self.orig_width],\n",
        "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    image = tf.image.random_crop(image,size=(self.new_height,\n",
        "                                              self.new_width,\n",
        "                                              3))\n",
        "    image = (tf.cast(image,tf.float32)/127.5) - 1\n",
        "    return image\n",
        "\n",
        "  def test_preprocess_image(self,image,label):\n",
        "    image = tf.image.resize(image, [self.new_height, self.new_width],\n",
        "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    image = (tf.cast(image,tf.float32)/127.5) - 1\n",
        "    return image\n",
        "\n",
        "  def data_generator(self,images,training=True):\n",
        "    if training:\n",
        "      dataset = images.map(self.test_preprocess_image,\n",
        "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    elif training == False:\n",
        "      dataset = images.map(self.train_preprocess_image,\n",
        "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.batch(self.batch_size)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "  def __call__(self):\n",
        "      train_horses, train_zebras, test_horses, test_zebras = self.load_data()\n",
        "      data_train_horses = self.data_generator(train_horses)\n",
        "      data_train_zebras = self.data_generator(train_zebras)\n",
        "      data_test_horses = self.data_generator(test_horses,\n",
        "                                             training=False)\n",
        "      data_test_zebras = self.data_generator(test_zebras,\n",
        "                                             training=False)\n",
        "      length_dataset = max(len(train_horses),len(train_zebras))\n",
        "\n",
        "      train_dataset = tf.data.Dataset.zip((data_train_horses,data_train_zebras))\n",
        "      test_dataset = tf.data.Dataset.zip((data_test_horses,data_test_zebras))\n",
        "\n",
        "      return train_dataset, test_dataset, data_train_horses, length_dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHERNjoJUga4"
      },
      "source": [
        "class Visualize:\n",
        "  def __init__(self,data,\n",
        "               generator_model_AB,\n",
        "               generator_model_BA):\n",
        "    self.data = data\n",
        "    self.generator_model_AB = generator_model_AB\n",
        "    self.generator_model_BA = generator_model_BA\n",
        "\n",
        "  def test_show_image(self):\n",
        "    plt.figure(figsize=(15,15))\n",
        "    for i in range(4):\n",
        "        for x in self.data.take(i+30):\n",
        "          img = tf.cast((tf.squeeze(x)+1)*127.5,tf.uint8)\n",
        "          plt.subplot(1,4,i+1)\n",
        "          plt.imshow(img)\n",
        "          plt.title(\"Original Image\")\n",
        "    plt.show()\n",
        "    \n",
        "    plt.figure(figsize=(15,15))\n",
        "    for i in range(4):\n",
        "      for x in self.data.take(i+30):\n",
        "        predicted_image = self.generator_model_AB(x)\n",
        "        plt.subplot(2,4,i+1)\n",
        "        plt.imshow(tf.cast((tf.squeeze(predicted_image)+1)*127.5,\n",
        "                           tf.uint8))\n",
        "        plt.title(\"Predicted Image\")\n",
        "    plt.show()  \n",
        "    \n",
        "    plt.figure(figsize=(15,15))\n",
        "    for i in range(4):\n",
        "      for x in self.data.take(i+30):\n",
        "        predicted_image = self.generator_model_AB(x)\n",
        "        plt.subplot(2,4,i+1)\n",
        "        predicted_image_back = self.generator_model_BA(predicted_image)\n",
        "        plt.imshow(tf.cast((tf.squeeze(predicted_image_back)+1)*127.5,\n",
        "                           tf.uint8))\n",
        "        plt.title(\"Reconstructed Image\")\n",
        "    plt.show()\n",
        "  \n",
        "  def __call__(self):\n",
        "    self.test_show_image()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4rzJKPIpUib"
      },
      "source": [
        "class Reflection_Pad(tf.keras.layers.Layer):\n",
        "  def __init__(self, pad):\n",
        "    super(Reflection_Pad,self).__init__()\n",
        "    self.pad = tuple(pad)\n",
        "  \n",
        "  def call(self, input_tensor):\n",
        "    pad_width, pad_height = self.pad\n",
        "    return tf.pad(input_tensor, \n",
        "                  [[0, 0],\n",
        "                   [pad_width, pad_height],\n",
        "                   [pad_width, pad_height],\n",
        "                   [0, 0]],\n",
        "                  mode = \"REFLECT\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44jzAajJ5lV-"
      },
      "source": [
        "class Resnet_Block(tf.keras.layers.Layer):\n",
        "  def __init__(self,filters):\n",
        "    super(Resnet_Block,self).__init__() \n",
        "    self.filters = filters\n",
        "    self.weight_init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
        "    self.reflection_pad_1 = Reflection_Pad(pad=(1,1))\n",
        "    self.reflection_pad_2 = Reflection_Pad(pad=(1,1))\n",
        "    self.conv_layer_1 = tf.keras.layers.Conv2D(self.filters,3,padding='valid',\n",
        "                                             kernel_initializer = self.weight_init)\n",
        "    self.conv_layer_2 = tf.keras.layers.Conv2D(self.filters,3,padding='valid',\n",
        "                                             kernel_initializer = self.weight_init)\n",
        "    self.instance_norm_1 = tfa.layers.InstanceNormalization(axis=-1)\n",
        "    self.instance_norm_2 = tfa.layers.InstanceNormalization(axis=-1)\n",
        "    self.activation = tf.keras.layers.ReLU()\n",
        "    self.add_layer = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self,input_tensor,training=None):\n",
        "    x = self.reflection_pad_1(input_tensor)\n",
        "    x = self.conv_layer_1(x)\n",
        "    x = self.instance_norm_1(x,training=training)\n",
        "    x = self.activation(x)\n",
        "    x = self.reflection_pad_2(x)\n",
        "    x = self.conv_layer_2(x)\n",
        "    x = self.instance_norm_2(x,training=training)\n",
        "    x = self.add_layer([input_tensor,x])\n",
        "    return x \n",
        "  \n",
        "class Generator(tf.keras.models.Model):\n",
        "  def __init__(self,filters,weights=None):\n",
        "    super(Generator,self).__init__()\n",
        "    self.filters = filters\n",
        "    self.weight_init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
        "    self.reflection_pad = Reflection_Pad(pad=(3,3))\n",
        "    \n",
        "    self.instance_norm1 = tfa.layers.InstanceNormalization(axis=-1)\n",
        "    self.instance_norm2 = tfa.layers.InstanceNormalization(axis=-1)\n",
        "    self.instance_norm3 = tfa.layers.InstanceNormalization(axis=-1)\n",
        "    self.instance_norm4 = tfa.layers.InstanceNormalization(axis=-1)\n",
        "    self.instance_norm5 = tfa.layers.InstanceNormalization(axis=-1)\n",
        "    self.relu = tf.keras.layers.ReLU()\n",
        "\n",
        "    self.conv_layer1 = tf.keras.layers.Conv2D(self.filters,7,padding='valid',\n",
        "                                             kernel_initializer=self.weight_init)\n",
        "    self.conv_layer2 = tf.keras.layers.Conv2D(self.filters*2,3,2,padding='same',\n",
        "                                              kernel_initializer=self.weight_init)\n",
        "    self.conv_layer3 = tf.keras.layers.Conv2D(self.filters*4,3,2,padding='same',\n",
        "                                                  kernel_initializer=self.weight_init)\n",
        "    self.make_resnet_block = self.make_resnet_blocks()\n",
        "    self.conv_transpose_layer4 = tf.keras.layers.Conv2DTranspose(self.filters*2,\n",
        "                                                                 3,2,padding='same',\n",
        "                                                                 kernel_initializer = self.weight_init)\n",
        "    self.conv_transpose_layer5 = tf.keras.layers.Conv2DTranspose(self.filters,3,2,padding='same',\n",
        "                                                                 kernel_initializer = self.weight_init)\n",
        "    self.conv_layer6 = tf.keras.layers.Conv2D(3,7,padding='same',\n",
        "                                              kernel_initializer=self.weight_init,\n",
        "                                              activation=\"tanh\")\n",
        "    if weights:\n",
        "      try:\n",
        "          self.load_weights(weights)\n",
        "      except Exception:\n",
        "          raise ValueError\n",
        "  \n",
        "  def make_resnet_blocks(self):\n",
        "    label = []\n",
        "    for _ in range(9):\n",
        "      label.append(Resnet_Block(self.filters*4))\n",
        "    return tf.keras.Sequential(label,name='residual_blocks')\n",
        "\n",
        "  def call(self,input_tensor,training=None):\n",
        "    x = self.reflection_pad(input_tensor)\n",
        "    x = self.conv_layer1(x)\n",
        "    x = self.instance_norm1(x,training=training)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv_layer2(x)\n",
        "    x = self.instance_norm2(x,training=training)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv_layer3(x)\n",
        "    x = self.instance_norm3(x,training=training)\n",
        "    x = self.relu(x)\n",
        "    x = self.make_resnet_block(x,training=training)\n",
        "    x = self.conv_transpose_layer4(x)\n",
        "    x = self.instance_norm4(x,training=training)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv_transpose_layer5(x)\n",
        "    x = self.instance_norm5(x,training=training)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv_layer6(x)\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkKJ4I3i9QL8"
      },
      "source": [
        "class Discriminator(tf.keras.layers.Layer):\n",
        "  def __init__(self,filters,**kwargs):\n",
        "    super(Discriminator,self).__init__()\n",
        "    self.filters = filters\n",
        "    self.weight_init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
        "    self.conv_layer1 = tf.keras.layers.Conv2D(self.filters,\n",
        "                                              4,2,padding='same',\n",
        "                                              kernel_initializer=self.weight_init)\n",
        "    self.instance_norm1 = tfa.layers.InstanceNormalization(axis=-1)\n",
        "    self.instance_norm2 = tfa.layers.InstanceNormalization(axis=-1)\n",
        "    self.instance_norm3 = tfa.layers.InstanceNormalization(axis=-1)\n",
        "    self.instance_norm4 = tfa.layers.InstanceNormalization(axis=-1)\n",
        "    self.l_relu = tf.keras.layers.LeakyReLU()\n",
        "    self.conv_layer2 = tf.keras.layers.Conv2D(self.filters*2,\n",
        "                                              4,2,padding='same',\n",
        "                                              kernel_initializer=self.weight_init)\n",
        "    self.conv_layer3 = tf.keras.layers.Conv2D(self.filters*4,\n",
        "                                              4,2,padding='same',\n",
        "                                              kernel_initializer=self.weight_init)\n",
        "    self.conv_layer4 = tf.keras.layers.Conv2D(self.filters*8,4,2,\n",
        "                                              padding='same',\n",
        "                                              kernel_initializer=self.weight_init)\n",
        "    self.conv_layer5 = tf.keras.layers.Conv2D(1,4,padding='same',\n",
        "                                              kernel_initializer=self.weight_init)\n",
        "  \n",
        "  def call(self,input_tensor):\n",
        "    x = self.conv_layer1(input_tensor)\n",
        "    x = self.l_relu(x)\n",
        "    x = self.conv_layer2(x)\n",
        "    x = self.instance_norm1(x)\n",
        "    x = self.l_relu(x)\n",
        "    x = self.conv_layer3(x)\n",
        "    x = self.instance_norm2(x)\n",
        "    x = self.l_relu(x)\n",
        "    x = self.conv_layer4(x)\n",
        "    x = self.instance_norm3(x)\n",
        "    x = self.l_relu(x)\n",
        "    patch_out = self.conv_layer5(x)\n",
        "    return patch_out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJjNVzGUh47v"
      },
      "source": [
        "class Cycle_GAN(tf.keras.models.Model):\n",
        "  def __init__(self,generator_AB=None,generator_BA=None,**kwargs):\n",
        "    super(Cycle_GAN,self).__init__()\n",
        "    self.discriminator_model_A = Discriminator(64)\n",
        "    self.discriminator_model_B = Discriminator(64)\n",
        "    self.generator_model_AB = Generator(32,weights=generator_AB)\n",
        "    self.generator_model_BA = Generator(32,weights=generator_BA)\n",
        "\n",
        "  def compile(self,generator_optimizer,discriminator_optimizer):\n",
        "    super(Cycle_GAN, self).compile()\n",
        "    self.generator_AB_optimizer = generator_optimizer\n",
        "    self.generator_BA_optimizer = generator_optimizer\n",
        "    self.discriminator_A_optimizer = discriminator_optimizer\n",
        "    self.discriminator_B_optimizer = discriminator_optimizer\n",
        "  \n",
        "  @staticmethod\n",
        "  def discriminator_loss(real,fake):\n",
        "    real_loss = tf.reduce_mean(tf.math.squared_difference(tf.ones_like(real),real))\n",
        "    fake_loss = tf.reduce_mean(tf.math.squared_difference(tf.zeros_like(fake),fake))\n",
        "    disc_loss = (real_loss+fake_loss)/2.0\n",
        "    return disc_loss\n",
        "  \n",
        "  @staticmethod\n",
        "  def identity_loss(real,identity):\n",
        "    loss = tf.reduce_mean(tf.abs(real-identity))\n",
        "    return loss*0.05\n",
        "\n",
        "  @staticmethod\n",
        "  def generator_loss(fake):\n",
        "    gen_loss = tf.reduce_mean(tf.math.squared_difference(tf.ones_like(fake),fake))\n",
        "    return gen_loss\n",
        "\n",
        "  @staticmethod\n",
        "  def cycle_loss(real,fake):\n",
        "    loss = tf.reduce_mean(tf.abs(real-fake))\n",
        "    return 10*loss\n",
        "  \n",
        "  def train_step(self,data):\n",
        "    real_a, real_b = data\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      fake_a = self.generator_model_BA(real_b,training=True)\n",
        "      fake_b = self.generator_model_AB(real_a,training=True)\n",
        "    \n",
        "      generated_a = self.generator_model_BA(fake_b,training=True)\n",
        "      generated_b = self.generator_model_AB(fake_a,training=True)\n",
        "\n",
        "      identity_a = self.generator_model_BA(real_a, training=True)\n",
        "      identity_b = self.generator_model_AB(real_b, training=True)\n",
        "\n",
        "      disc_real_b = self.discriminator_model_B(real_b,training=True)\n",
        "      disc_fake_b = self.discriminator_model_B(fake_b,training=True)\n",
        "\n",
        "      disc_real_a = self.discriminator_model_A(real_a,training=True)\n",
        "      disc_fake_a = self.discriminator_model_A(fake_a,training=True)\n",
        "      \n",
        "      identity_loss_A = self.identity_loss(real_b,identity_b)\n",
        "      identity_loss_B = self.identity_loss(real_a,identity_a)\n",
        "      cycle_loss_A = self.cycle_loss(real_b,generated_b)\n",
        "      cycle_loss_B = self.cycle_loss(real_a,generated_a)\n",
        "      gen_loss_A = self.generator_loss(disc_fake_b)\n",
        "      gen_loss_B = self.generator_loss(disc_fake_a)\n",
        "      \n",
        "      generator_loss_A = gen_loss_A+cycle_loss_A+identity_loss_A\n",
        "      generator_loss_B = gen_loss_B+cycle_loss_B+identity_loss_B\n",
        "\n",
        "      disc_loss_A = self.discriminator_loss(disc_real_a,disc_fake_a)\n",
        "      disc_loss_B = self.discriminator_loss(disc_real_b,disc_fake_b)\n",
        "\n",
        "    \n",
        "    generator_AB_gradients = tape.gradient(generator_loss_A,\n",
        "                                           self.generator_model_AB.\\\n",
        "                                           trainable_variables)\n",
        "    generator_BA_gradients = tape.gradient(generator_loss_B,\n",
        "                                           self.generator_model_BA.\\\n",
        "                                           trainable_variables)\n",
        "\n",
        "    discriminator_A_gradients = tape.gradient(disc_loss_A,\n",
        "                                              self.discriminator_model_A.\\\n",
        "                                              trainable_variables)\n",
        "    discriminator_B_gradients = tape.gradient(disc_loss_B,\n",
        "                                              self.discriminator_model_B.\\\n",
        "                                              trainable_variables)\n",
        "\n",
        "    self.generator_AB_optimizer.apply_gradients(zip(generator_AB_gradients,\n",
        "                                                    self.generator_model_AB.\\\n",
        "                                                    trainable_variables))\n",
        "    self.generator_BA_optimizer.apply_gradients(zip(generator_BA_gradients,\n",
        "                                                    self.generator_model_BA.\\\n",
        "                                                    trainable_variables))\n",
        "    self.discriminator_A_optimizer.apply_gradients(zip(discriminator_A_gradients,\n",
        "                                                       self.discriminator_model_A.\\\n",
        "                                                       trainable_variables))\n",
        "    self.discriminator_B_optimizer.apply_gradients(zip(discriminator_B_gradients,\n",
        "                                                       self.discriminator_model_B.\\\n",
        "                                                       trainable_variables))\n",
        "    return {\"generator_loss_A\":generator_loss_A,\n",
        "            \"generator_loss_B\":generator_loss_B,\n",
        "            \"disc_loss_A\":disc_loss_A,\n",
        "            \"disc_loss_B\":disc_loss_B}\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpBFPgzof1lE"
      },
      "source": [
        "class Linear_Decay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, initial_learning_rate, total_steps, step_decay):\n",
        "    super(Linear_Decay, self).__init__()\n",
        "    self.initial_learning_rate = initial_learning_rate\n",
        "    self.total_steps = total_steps\n",
        "    self.step_decay = step_decay\n",
        "    self.current_learning_rate = tf.Variable(initial_value=initial_learning_rate,\n",
        "                                             trainable=False, dtype=tf.float32)\n",
        "  def __call__(self, step):\n",
        "    if step >= self.step_decay:\n",
        "      return self.initial_learning_rate * (1 - 1 / (self.total_steps - \n",
        "                                                           self.step_decay) * \n",
        "                                                  (step - self.step_decay))\n",
        "    else:\n",
        "      return initial_learning_rate"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZblgdjsKifjA"
      },
      "source": [
        "def main():\n",
        "  batch_size = 1\n",
        "  epochs = 50\n",
        "  epoch_decay = 100\n",
        "  learning_rate = 0.0002\n",
        "  image_width = 256\n",
        "  image_height = 256\n",
        "\n",
        "  save_path = \"/content/drive/MyDrive/cycle_gan/\"\n",
        "  weights_dir_AB = \"/content/drive/MyDrive/cycle_gan_AB/\"\n",
        "  weights_dir_BA = \"/content/drive/MyDrive/cycle_gan_BA/\"\n",
        "  \n",
        "  data = InputPipeline()\n",
        "  train_dataset, test_dataset, data_train_horses, length_dataset = data.__call__()\n",
        "\n",
        "  cycle_gan = Cycle_GAN()\n",
        "  cycle_gan.discriminator_model_B.build((1,image_height,\n",
        "                                         image_width,3))\n",
        "  cycle_gan.discriminator_model_A.build((1,image_height,\n",
        "                                         image_width,3))\n",
        "  cycle_gan.generator_model_AB.build((1,image_height,\n",
        "                                      image_width,3))\n",
        "  cycle_gan.generator_model_BA.build((1,image_height,\n",
        "                                      image_width,3))\n",
        "\n",
        "  generator_learning_rate = Linear_Decay(learning_rate,epochs*length_dataset,\n",
        "                                        epoch_decay*length_dataset)\n",
        "  discriminator_learning_rate = Linear_Decay(learning_rate,epochs*length_dataset,\n",
        "                                            epoch_decay*length_dataset)\n",
        "  cycle_gan.compile(generator_optimizer=tf.keras.optimizers.Adam(learning_rate,\n",
        "                                                                 beta_1=0.5),\n",
        "                    discriminator_optimizer=tf.keras.optimizers.Adam(learning_rate,\n",
        "                                                                     beta_1=0.5))\n",
        "  callbacks = [\n",
        "      tf.keras.callbacks.ModelCheckpoint(\n",
        "          save_path,\n",
        "          save_weights_only=True,\n",
        "          save_best_only=True,\n",
        "          monitor='generator_loss_A',\n",
        "          verbose=1,\n",
        "          save_freq='epoch',\n",
        "          period=1),\n",
        "  ]\n",
        "  \n",
        "  cycle_gan.fit(train_dataset,batch_size=batch_size,steps_per_epoch=length_dataset//batch_size,\n",
        "              epochs=epochs,callbacks=callbacks)\n",
        "  \n",
        "  cycle_gan.generator_model_AB.save_weights(weights_dir_AB)\n",
        "  cycle_gan.generator_model_BA.save_weights(weights_dir_BA)\n",
        "  \n",
        "  visualize = Visualize(data_train_horses,\n",
        "                        cycle_gan.generator_model_AB,\n",
        "                        cycle_gan.generator_model_BA)\n",
        "  visualize.__call__()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}