{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SavingLoadingModels.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATZEV0LIz1Ax"
      },
      "source": [
        "## Importing Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ssfsU0ny9Vi"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXoQgDej3SFX"
      },
      "source": [
        "## Saving and loading from weights \n",
        "- Used when model architecture, configurations, optimizer and states is already available.\n",
        "- If model is saved with model sub classing, then it should be loaded with model subclassing only. It can't be loaded via functional or sequential api.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtkEDUeuz6Ao",
        "outputId": "b67a4263-6b15-434b-eac9-9632d0d9ac3d"
      },
      "source": [
        "def data_generator(x,y,batch_size,epochs):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x/255.0,tf.float32),tf.cast(y,tf.int32)))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.repeat(epochs)\n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset\n",
        "\n",
        "class Basic_model(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Basic_model,self).__init__()\n",
        "    self.kernel_init = tf.keras.initializers.glorot_normal()\n",
        "    self.conv = tf.keras.layers.Conv2D(64,3,kernel_initializer=self.kernel_init)\n",
        "    self.bn = tf.keras.layers.BatchNormalization()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.out = tf.keras.layers.Dense(10,activation='softmax')\n",
        "  \n",
        "  def call(self,input_tensor):\n",
        "    x = self.conv(input_tensor)\n",
        "    x = self.bn(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.out(x)\n",
        "    return x\n",
        "\n",
        "  def compile(self, optimizer,loss):\n",
        "      super(Basic_model, self).compile()\n",
        "      self.optimizer = optimizer\n",
        "      self.loss = loss\n",
        "\n",
        "  def train_step(self,input_data):\n",
        "    input,output = input_data\n",
        "    with tf.GradientTape() as t:\n",
        "      y_pred = self(input, training = True)\n",
        "      loss_val = self.loss(output,y_pred)\n",
        "    grad = t.gradient(loss_val,self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(grad,self.trainable_variables))\n",
        "    return {\"loss\":loss_val}\n",
        "\n",
        "\n",
        "def main():\n",
        "  batch_size = 128\n",
        "  epochs = 1\n",
        "\n",
        "  (x_train,y_train),(x_test,y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "  train_dataset = data_generator(x_train,y_train,batch_size,epochs)\n",
        "  test_dataset = data_generator(x_test,y_test,batch_size,epochs)\n",
        "  \n",
        "  model = Basic_model()\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  model.compile(optimizer=optimizer, loss=loss)\n",
        "  model.fit(train_dataset,epochs=epochs,batch_size=batch_size)\n",
        "  #Saving model weights\n",
        "  model.save_weights('Basic_model_weights/')\n",
        "  model.load_weights('Basic_model_weights/')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "391/391 [==============================] - 33s 8ms/step - loss: 3.1090\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdORcjHH-XxP"
      },
      "source": [
        "## Saving and loading from checkpoint\n",
        "- Used when we want to save model in between the training process.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__7UFXE099G8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b713d7-2b7a-4b96-ddbc-d0717e0b12b9"
      },
      "source": [
        "def data_generator(x,y,batch_size,epochs):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x/255.0,tf.float32),tf.cast(y,tf.int32)))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.repeat(epochs)\n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset\n",
        "\n",
        "class Basic_model(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Basic_model,self).__init__()\n",
        "    self.kernel_init = tf.keras.initializers.glorot_normal()\n",
        "    self.conv = tf.keras.layers.Conv2D(64,3,kernel_initializer=self.kernel_init)\n",
        "    self.bn = tf.keras.layers.BatchNormalization()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.out = tf.keras.layers.Dense(10,activation='softmax')\n",
        "  \n",
        "  def call(self,input_tensor):\n",
        "    x = self.conv(input_tensor)\n",
        "    x = self.bn(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.out(x)\n",
        "    return x\n",
        "\n",
        "  def compile(self, optimizer,loss):\n",
        "      super(Basic_model, self).compile()\n",
        "      self.optimizer = optimizer\n",
        "      self.loss = loss\n",
        "\n",
        "  def train_step(self,input_data):\n",
        "    input,output = input_data\n",
        "    with tf.GradientTape() as t:\n",
        "      y_pred = self(input, training = True)\n",
        "      loss_val = self.loss(output,y_pred)\n",
        "    grad = t.gradient(loss_val,self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(grad,self.trainable_variables))\n",
        "    return {\"loss\":loss_val}\n",
        "\n",
        "\n",
        "def main():\n",
        "  batch_size = 128\n",
        "  epochs = 2\n",
        "\n",
        "  (x_train,y_train),(x_test,y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "  train_dataset = data_generator(x_train,y_train,batch_size,epochs)\n",
        "  test_dataset = data_generator(x_test,y_test,batch_size,epochs)  \n",
        "\n",
        "  model = Basic_model()\n",
        "  #For loading the model from latest checkpoint\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "  model_save_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='Basic_model_checkpnt_/'+str(epochs),\n",
        "                                                             monitor='loss',\n",
        "                                                             mode='min',\n",
        "                                                             save_best_only=True,\n",
        "                                                             save_weights=True\n",
        "                                                             )\n",
        "  model.fit(train_dataset,epochs=epochs,batch_size=batch_size,callbacks=[model_save_checkpoint])\n",
        "  basic_model_latest_checkpoint = tf.train.latest_checkpoint('Basic_model_checkpnt_/')\n",
        "  model.load_weights(basic_model_latest_checkpoint)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 2.4697\n",
            "Epoch 2/2\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.7744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99ys4zpeHQfP"
      },
      "source": [
        "## Saving and loading from serialization\n",
        "- Used when we want to whole save model with config, weights, optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9766MZTwGpzb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "957ff7c4-924b-4668-8f87-0da2c04ee342"
      },
      "source": [
        "def data_generator(x,y,batch_size,epochs):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x/255.0,tf.float32),tf.cast(y,tf.int32)))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.repeat(epochs)\n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset\n",
        "\n",
        "class Basic_model(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Basic_model,self).__init__()\n",
        "    self.kernel_init = tf.keras.initializers.glorot_normal()\n",
        "    self.conv = tf.keras.layers.Conv2D(64,3,kernel_initializer=self.kernel_init)\n",
        "    self.bn = tf.keras.layers.BatchNormalization()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.out = tf.keras.layers.Dense(10,activation='softmax')\n",
        "  \n",
        "  def call(self,input_tensor):\n",
        "    x = self.conv(input_tensor)\n",
        "    x = self.bn(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.out(x)\n",
        "    return x\n",
        "\n",
        "  def compile(self, optimizer,loss):\n",
        "      super(Basic_model, self).compile()\n",
        "      self.optimizer = optimizer\n",
        "      self.loss = loss\n",
        "\n",
        "  def train_step(self,input_data):\n",
        "    input,output = input_data\n",
        "    with tf.GradientTape() as t:\n",
        "      y_pred = self(input, training = True)\n",
        "      loss_val = self.loss(output,y_pred)\n",
        "    grad = t.gradient(loss_val,self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(grad,self.trainable_variables))\n",
        "    return {\"loss\":loss_val}\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "  batch_size = 128\n",
        "  epochs = 1\n",
        "\n",
        "  (x_train,y_train),(x_test,y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "  train_dataset = data_generator(x_train,y_train,batch_size,epochs)\n",
        "  test_dataset = data_generator(x_test,y_test,batch_size,epochs)  \n",
        "  \n",
        "  model = Basic_model()\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  model.compile(optimizer=optimizer, loss=loss)\n",
        "  model.fit(train_dataset,epochs=epochs,batch_size=batch_size)\n",
        "  model.save('Basic_model_complete/')\n",
        "\n",
        "  #When loading the model for inference, we just need to load from the saved directory\n",
        "  new_model = tf.keras.models.load_model('Basic_model_complete/')\n",
        "  new_model.summary()\n",
        "  new_model.get_weights()\n",
        "  print(\"optimizer\",new_model.optimizer)\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 3s 8ms/step - loss: 3.0716\n",
            "INFO:tensorflow:Assets written to: Basic_model_complete/assets\n",
            "Model: \"basic_model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            multiple                  1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch multiple                  256       \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  576010    \n",
            "=================================================================\n",
            "Total params: 578,058\n",
            "Trainable params: 577,930\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n",
            "optimizer <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5cca66e550>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}