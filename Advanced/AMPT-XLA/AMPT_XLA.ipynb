{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AMPT-XLA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DfF5Um1SKym",
        "outputId": "17b0cc7c-0c3e-4920-88d5-b9b07b066d68"
      },
      "source": [
        "import os\n",
        "from time import time\n",
        "import tensorflow as tf\n",
        "\n",
        "def data_generator(features,labels,batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((tf.cast((features/255),tf.float32),labels))\n",
        "  dataset = dataset.shuffle(buffer_size=len(labels)+1)\n",
        "  dataset = dataset.batch(batch_size=batch_size,\n",
        "                          drop_remainder=True)\n",
        "  dataset = dataset.repeat()\n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset\n",
        "\n",
        "def model():  \n",
        "  model = tf.keras.applications.Xception(include_top=False,\n",
        "                                            weights='imagenet')\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(model.output)\n",
        "  predictions = tf.keras.layers.Dense(num_classes,activation=\"softmax\")(x)\n",
        "  return tf.keras.Model(inputs=model.input, outputs=predictions)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  batch_size = 32\n",
        "  epochs = 2\n",
        "  num_classes = 10\n",
        "\n",
        "  (x_train,y_train),(x_test,y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "  train_dataset = data_generator(x_train, y_train, batch_size)\n",
        "  test_dataset = data_generator(x_test, y_test, batch_size)\n",
        "\n",
        "  xception = model()\n",
        "  xception.compile(loss='sparse_categorical_crossentropy',optimizer=\"adam\",metrics=[\"accuracy\"])\n",
        "\n",
        "  start_time = time()\n",
        "  xception.fit(train_dataset, epochs=epochs, steps_per_epoch=len(x_train)//batch_size,\n",
        "              validation_data=test_dataset,validation_steps=len(x_test)//batch_size)\n",
        "  end_time = time()\n",
        "\n",
        "  print(\"Time without xla and mpt\",end_time-start_time)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1562/1562 [==============================] - 101s 61ms/step - loss: 1.8690 - accuracy: 0.2933 - val_loss: 0.8928 - val_accuracy: 0.6964\n",
            "Epoch 2/2\n",
            "1562/1562 [==============================] - 94s 60ms/step - loss: 0.8278 - accuracy: 0.7258 - val_loss: 0.7259 - val_accuracy: 0.7563\n",
            "Time without xla and mpt 195.7514898777008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOaVzshzSny9",
        "outputId": "90889975-37d6-483b-aace-53a35d102beb"
      },
      "source": [
        "import os\n",
        "from time import time\n",
        "import tensorflow as tf\n",
        "\n",
        "def data_generator(features,labels,batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((tf.cast((features/255),tf.float16),tf.cast(labels,tf.float16)))\n",
        "  dataset = dataset.shuffle(buffer_size=len(labels)+1)\n",
        "  dataset = dataset.batch(batch_size=batch_size,\n",
        "                          drop_remainder=True)\n",
        "  dataset = dataset.repeat()\n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset\n",
        "\n",
        "def model():  \n",
        "  model = tf.keras.applications.Xception(include_top=False,\n",
        "                                            weights='imagenet')\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(model.output)\n",
        "  x = tf.keras.layers.Dense(num_classes)(x)\n",
        "  predictions = tf.keras.layers.Activation('softmax', dtype=tf.float32)(x)\n",
        "  return tf.keras.Model(inputs=model.input, outputs=predictions)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  os.environ['TF_XLA_FLAGS'] = \"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit\"\n",
        "  tf.config.optimizer.set_jit(True)\n",
        "\n",
        "  policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
        "  tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "\n",
        "  batch_size = 32\n",
        "  epochs = 2\n",
        "  num_classes = 10\n",
        "\n",
        "  (x_train,y_train),(x_test,y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "  train_dataset = data_generator(x_train, y_train, batch_size)\n",
        "  test_dataset = data_generator(x_test, y_test, batch_size)\n",
        "\n",
        "  optimizer = tf.optimizers.Adam()\n",
        "  optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, loss_scale='dynamic')\n",
        "\n",
        "  xception = model()\n",
        "  xception.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer,metrics=[\"accuracy\"])\n",
        "\n",
        "  start_time = time()\n",
        "  xception.fit(train_dataset, epochs=epochs, steps_per_epoch=len(x_train)//batch_size,\n",
        "              validation_data=test_dataset,validation_steps=len(x_test)//batch_size)\n",
        "  end_time = time()\n",
        "  print(\"Time with xla and mpt\",end_time-start_time)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
            "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla T4, compute capability 7.5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n",
            "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. For example\n",
            "  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\n",
            "Epoch 1/2\n",
            "1562/1562 [==============================] - 67s 28ms/step - loss: 2.3043 - accuracy: 0.1024 - val_loss: 2.3062 - val_accuracy: 0.0955\n",
            "Epoch 2/2\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 2.3051 - accuracy: 0.1021 - val_loss: 2.3041 - val_accuracy: 0.0978\n",
            "Time with xla and mpt 107.1470365524292\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}